"""
Coherent Optical Neural Network (CoherentONN)

Implementaci√≥n de red neuronal √≥ptica coherente basada en:
üìö Shen et al. "Deep learning with coherent nanophotonic circuits" (Nature Photonics 2017)

üéØ Arquitectura:
- Cada capa = Matriz unitaria implementada con mesh de MZIs
- Activaciones = Detecci√≥n √≥ptica + re-codificaci√≥n 
- Conservaci√≥n de energ√≠a garantizada
- Entrenamiento end-to-end con PyTorch

üîß Componentes Usados (OpticalCI existentes):
- MZILayer / MZIBlockLinear: Para matrices unitarias
- Photodetector: Para activaciones no-lineales
- Device management: Heredado de ONNBaseModel

‚ö†Ô∏è  IMPORTANTE: Solo usa componentes existentes, no modifica nada.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import List, Optional, Union, Tuple, Dict, Any
import warnings

# Imports de componentes OpticalCI existentes (NO MODIFICAR)
from ...layers import MZILayer, MZIBlockLinear, Photodetector
from .base_onn import BaseONN


class CoherentONN(BaseONN):
    """
    Coherent Optical Neural Network usando mesh de MZIs.
    
    Implementa la arquitectura propuesta por Shen et al. (2017):
    1. Cada capa linear = Matriz unitaria (MZI mesh)  
    2. Activaci√≥n = Photodetection + optical re-encoding
    3. Clasificaci√≥n = Capa final el√©ctrica
    
    Caracter√≠sticas:
    ‚úÖ Matrices estrictamente unitarias (conservaci√≥n de energ√≠a)
    ‚úÖ F√≠sica realista usando componentes OpticalCI
    ‚úÖ Entrenamiento compatible con PyTorch
    ‚úÖ Validaci√≥n autom√°tica de propiedades f√≠sicas
    """
    
    def __init__(
        self,
        layer_sizes: List[int],
        activation_type: str = "square_law",
        optical_power: float = 1.0,
        use_unitary_constraints: bool = True,
        device: Optional[Union[str, torch.device]] = None
    ):
        """
        Inicializar CoherentONN.
        
        Args:
            layer_sizes: Lista con tama√±os de capas [input, hidden1, hidden2, ..., output]
            activation_type: Tipo de activaci√≥n ("square_law", "linear")
            optical_power: Potencia √≥ptica normalizada
            use_unitary_constraints: Usar restricciones unitarias estrictas
            device: Device (CPU/GPU)
        """
        super().__init__(
            device=device,
            optical_power=optical_power,
            wavelength_channels=1,  # Coherent ONN usa 1 canal
            enable_physics_validation=True
        )
        
        self.layer_sizes = layer_sizes
        self.activation_type = activation_type
        self.use_unitary_constraints = use_unitary_constraints
        self.n_layers = len(layer_sizes) - 1
        
        if len(layer_sizes) < 2:
            raise ValueError("Need at least input and output layers")
        
        print(f"üåü CoherentONN Architecture:")
        print(f"   Layers: {' ‚Üí '.join(map(str, layer_sizes))}")
        print(f"   Activation: {activation_type}")
        print(f"   Unitary constraints: {use_unitary_constraints}")
        print(f"   Total parameters: {self._count_parameters()}")
        
        # Crear capas √≥pticas unitarias
        self.optical_layers = nn.ModuleList()
        self.photodetectors = nn.ModuleList()
        
        for i in range(self.n_layers):  # CORRECCI√ìN: Todas las capas incluyendo la √∫ltima
            in_size = layer_sizes[i]
            out_size = layer_sizes[i + 1]
            
            # Capa √≥ptica unitaria (solo para capas intermedias)
            if i < self.n_layers - 1:
                if use_unitary_constraints:
                    # Usar MZILayer para garantizar unitaridad estricta
                    optical_layer = MZILayer(
                        in_features=in_size,
                        out_features=out_size,
                        device=self.device
                    )
                else:
                    # Usar MZIBlockLinear modo USV para flexibilidad
                    optical_layer = MZIBlockLinear(
                        in_features=in_size,
                        out_features=out_size,
                        mode="usv",  # USV permite aproximar cualquier matriz
                        device=self.device
                    )
                
                self.optical_layers.append(optical_layer)
            
            # Photodetector para TODAS las capas (incluyendo la final)
            photodetector = Photodetector(
                responsivity=1.0,  # Normalizado
                dark_current=0.0,  # Ideal para simulaci√≥n
                device=self.device
            )
            self.photodetectors.append(photodetector)
        
        # Capa final: el√©ctrica para clasificaci√≥n
        final_in = layer_sizes[-2]
        final_out = layer_sizes[-1]
        self.final_layer = nn.Linear(final_in, final_out, device=self.device)
        
        # Inicializaci√≥n
        self._initialize_parameters()
    
    def _count_parameters(self) -> int:
        """Estimar n√∫mero de par√°metros para cada configuraci√≥n."""
        total = 0
        for i in range(len(self.layer_sizes) - 1):
            in_size = self.layer_sizes[i]
            out_size = self.layer_sizes[i + 1]
            
            if self.use_unitary_constraints:
                # MZI Layer: n√∫mero de par√°metros seg√∫n descomposici√≥n de Reck
                max_dim = max(in_size, out_size)
                n_mzis = max_dim * (max_dim - 1) // 2
                n_phases = max_dim
                total += n_mzis * 2 + n_phases  # theta, phi_internal, phi_external
            else:
                # MZIBlockLinear USV mode
                total += in_size * out_size  # Aproximaci√≥n
        
        # Capa final
        total += self.layer_sizes[-2] * self.layer_sizes[-1]
        
        return total
    
    def _initialize_parameters(self):
        """Inicializar par√°metros con valores apropiados para gradientes estables."""
        with torch.no_grad():
            # Inicializar capas √≥pticas con valores m√°s peque√±os para estabilidad
            for optical_layer in self.optical_layers:
                if hasattr(optical_layer, 'reset_parameters'):
                    optical_layer.reset_parameters()
                
                # Si tiene par√°metros de fase, inicializar con valores peque√±os
                for name, param in optical_layer.named_parameters():
                    if 'phase' in name.lower() or 'phi' in name.lower():
                        # Fases peque√±as para evitar saturaci√≥n inicial
                        nn.init.uniform_(param, -0.1, 0.1)
                    elif 'theta' in name.lower():
                        # √Ångulos de splitting peque√±os
                        nn.init.uniform_(param, -0.2, 0.2)
            
            # Inicializar capa final con ganancia m√°s peque√±a
            nn.init.xavier_uniform_(self.final_layer.weight, gain=0.5)
            if self.final_layer.bias is not None:
                nn.init.zeros_(self.final_layer.bias)
    
    def _apply_optical_activation(
        self, 
        optical_signal: torch.Tensor, 
        layer_idx: int
    ) -> torch.Tensor:
        """
        Aplicar activaci√≥n √≥ptica con gradientes estables.
        
        Args:
            optical_signal: Se√±al √≥ptica coherente [batch_size, features]
            layer_idx: √çndice de la capa (para acceder al photodetector)
            
        Returns:
            Se√±al activada re-codificada √≥pticamente
        """
        # 1. Photodetecci√≥n (convierte a intensidad)
        photodetector = self.photodetectors[layer_idx]
        electrical_signal = photodetector(optical_signal)
        
        # 2. Aplicar funci√≥n de activaci√≥n
        if self.activation_type == "square_law":
            # Ya aplicada por el photodetector (|E|¬≤)
            activated = electrical_signal
        elif self.activation_type == "linear":
            # Activaci√≥n lineal (no cambio)
            activated = electrical_signal
        elif self.activation_type == "relu":
            # ReLU el√©ctrico
            activated = F.relu(electrical_signal)
        elif self.activation_type == "tanh":
            # Tanh el√©ctrico (m√°s dif√≠cil de implementar √≥pticamente)
            activated = torch.tanh(electrical_signal)
        else:
            raise ValueError(f"Unknown activation type: {self.activation_type}")
        
        # 3. Re-codificar √≥pticamente (MEJORADO para gradientes estables)
        # Problema: sqrt() puede dar gradientes infinitos cerca de 0
        # Soluci√≥n: usar clamp con epsilon m√°s alto y aplicar suavizado
        
        epsilon = 1e-6  # Evitar divisiones por cero en gradientes
        activated_clamped = torch.clamp(activated, min=epsilon)
        
        # Alternativa m√°s estable que sqrt puro:
        # Usar una funci√≥n suave que se comporta como sqrt pero con mejores gradientes
        if self.training:
            # Durante entrenamiento: funci√≥n m√°s suave
            optical_reencoded = torch.pow(activated_clamped, 0.4)  # x^0.4 en vez de x^0.5
        else:
            # Durante inferencia: sqrt normal para precisi√≥n f√≠sica
            optical_reencoded = torch.sqrt(activated_clamped)
        
        # Normalizaci√≥n suave para mantener gradientes
        current_power = torch.sum(optical_reencoded**2, dim=1, keepdim=True)
        target_power = self.optical_power
        
        # Normalizaci√≥n suave sin torch.where (mejor para gradientes)
        power_ratio = current_power / (target_power + epsilon)
        
        # Usar soft clamping en lugar de hard thresholding
        # Esto mantiene gradientes suaves en todos los casos
        soft_clamp_factor = torch.sigmoid((power_ratio - 1.5) * 4.0)  # Suave around 1.5
        normalization_strength = soft_clamp_factor * 0.5  # Max 50% correction
        
        normalization_factor = 1.0 - normalization_strength + normalization_strength * torch.sqrt(target_power / (current_power + epsilon))
        optical_reencoded = optical_reencoded * normalization_factor
        
        return optical_reencoded
    
    def _forward_optical(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass √≥ptico implementando la arquitectura coherente.
        
        Flujo:
        1. Input ‚Üí Campo √≥ptico
        2. Para cada capa: MZI mesh ‚Üí Photodetection ‚Üí Re-encoding  
        3. Final layer ‚Üí Clasificaci√≥n el√©ctrica
        """
        batch_size = x.size(0)
        
        # 1. Convertir input a campo √≥ptico (gradient-safe)
        # Usar operaciones que mantengan gradientes suaves
        epsilon = 1e-6
        
        # Normalizaci√≥n suave que mantiene gradientes
        x_min = torch.min(x)
        x_max = torch.max(x)
        x_range = x_max - x_min + epsilon
        
        # Shift and scale suavemente
        x_shifted = x - x_min + epsilon  # Ensure positive
        x_normalized = x_shifted / (x_range + epsilon)  # [0, 1]
        
        # Asegurar que est√© en [0, 1] de manera diferenciable
        x_clamped = torch.sigmoid(x_normalized * 4.0 - 2.0)  # Soft clamp to [0,1]
        
        # Convertir a campo √≥ptico de manera estable
        optical_field = torch.sqrt(x_clamped * self.optical_power + epsilon)
        
        # 2. Procesar a trav√©s de capas √≥pticas
        current_signal = optical_field
        
        # Procesar capas intermedias con MZI + activaci√≥n
        for i, optical_layer in enumerate(self.optical_layers):
            # Aplicar transformaci√≥n unitaria (MZI mesh)
            try:
                current_signal = optical_layer(current_signal)
            except Exception as e:
                warnings.warn(f"Layer {i} forward failed: {e}")
                # Fallback: mantener se√±al sin cambio
                pass
            
            # Aplicar activaci√≥n √≥ptica (photodetection + re-encoding)
            current_signal = self._apply_optical_activation(current_signal, i)
            
            # Validaci√≥n f√≠sica ocasional
            if self.training and torch.rand(1).item() < 0.05:  # 5% de las veces
                signal_power = torch.sum(torch.abs(current_signal)**2, dim=1)
                max_power = torch.max(signal_power)
                if max_power > self.optical_power * 1.5:
                    warnings.warn(f"Layer {i}: Optical power exceeded: {max_power:.3f}")
        
        # 3. Conversi√≥n final a se√±al el√©ctrica usando el √∫ltimo photodetector
        # CORRECCI√ìN: Verificar que existe el photodetector
        if len(self.photodetectors) > 0:
            final_photodetector = self.photodetectors[-1]
            electrical_output = final_photodetector(current_signal)
        else:
            # Fallback si no hay photodetectors (no deber√≠a pasar)
            warnings.warn("No photodetectors available, using intensity conversion")
            electrical_output = torch.abs(current_signal)**2
        
        # 4. Capa de clasificaci√≥n el√©ctrica final
        logits = self.final_layer(electrical_output)
        
        # Debug: Verificar que electrical_output mantenga gradientes
        if self.training and electrical_output.requires_grad:
            def hook_fn(grad):
                if torch.any(torch.isnan(grad)):
                    print(f"   üêõ NaN gradient detected at electrical_output")
                return grad
            electrical_output.register_hook(hook_fn)
        
        return logits
    
    def get_optical_efficiency(self) -> Dict[str, float]:
        """
        Calcular m√©tricas de eficiencia √≥ptica.
        
        Returns:
            Dict con m√©tricas de eficiencia
        """
        efficiency = {}
        
        # M√©tricas b√°sicas
        efficiency["n_optical_layers"] = len(self.optical_layers)
        efficiency["n_photodetectors"] = len(self.photodetectors)
        efficiency["theoretical_speedup"] = len(self.optical_layers)  # Vs. electronic
        
        # N√∫mero de operaciones √≥pticas vs el√©ctricas
        optical_ops = sum(
            layer.in_features * layer.out_features 
            for layer in self.optical_layers
        )
        electrical_ops = self.final_layer.in_features * self.final_layer.out_features
        
        efficiency["optical_operations"] = optical_ops
        efficiency["electrical_operations"] = electrical_ops
        efficiency["optical_fraction"] = optical_ops / (optical_ops + electrical_ops)
        
        return efficiency
    
    def validate_unitarity(self) -> Dict[str, Any]:
        """
        Validar propiedades unitarias de las capas √≥pticas.
        
        Returns:
            Dict con resultados de validaci√≥n
        """
        validation = {"layers": {}, "overall_valid": True}
        
        for i, layer in enumerate(self.optical_layers):
            layer_validation = {"is_unitary": False, "error": float('inf')}
            
            try:
                if hasattr(layer, 'get_unitary_matrix'):
                    # Para MZILayer
                    U = layer.get_unitary_matrix()
                    identity_check = torch.matmul(U, torch.conj(U.t()))
                    identity_target = torch.eye(U.size(0), dtype=U.dtype, device=U.device)
                    error = torch.max(torch.abs(identity_check - identity_target)).item()
                    
                    layer_validation["is_unitary"] = error < 1e-3
                    layer_validation["unitarity_error"] = error
                elif hasattr(layer, '_get_weight_matrix'):
                    # Para MZIBlockLinear, verificar que ||W||_2 ‚âà 1
                    W = layer._get_weight_matrix()
                    singular_values = torch.svd(W)[1]
                    max_sv = torch.max(singular_values).item()
                    
                    layer_validation["is_unitary"] = max_sv <= 1.1  # Permitir cierta tolerancia
                    layer_validation["max_singular_value"] = max_sv
            except Exception as e:
                layer_validation["error"] = str(e)
            
            validation["layers"][f"layer_{i}"] = layer_validation
            
            if not layer_validation.get("is_unitary", False):
                validation["overall_valid"] = False
        
        return validation
    
    def extra_repr(self) -> str:
        """Representaci√≥n adicional para debugging."""
        base_repr = super().extra_repr()
        coherent_repr = (f"layer_sizes={self.layer_sizes}, "
                        f"activation_type='{self.activation_type}', "
                        f"unitary_constraints={self.use_unitary_constraints}")
        
        return f"{base_repr}, {coherent_repr}"


def create_simple_coherent_onn(
    input_size: int = 4,
    hidden_size: int = 8, 
    output_size: int = 3,
    device: Optional[torch.device] = None
) -> CoherentONN:
    """
    Crear una CoherentONN simple para testing y demos.
    
    Args:
        input_size: Tama√±o de entrada
        hidden_size: Tama√±o de capa oculta
        output_size: Tama√±o de salida
        device: Device
        
    Returns:
        CoherentONN configurada y lista para usar
    """
    return CoherentONN(
        layer_sizes=[input_size, hidden_size, output_size],
        activation_type="square_law",
        optical_power=1.0,
        use_unitary_constraints=True,
        device=device
    )